The paper "Large Language Models to Enhance Bayesian Optimization" [arXiv:2402.03921] explores the utilization of Large Language Models (LLMs) to improve the performance of Bayesian Optimization (BO) for Hyperparameter Optimization (HPO). The primary focus is on utilizing LLMs to generate better initialization points (warmstarting), to enhance surrogate modeling as well as candidate sampling. 

Building on the insights from this paper, our research aims to test the capabilities of **LLMs in Neural Architecture Search in a multi-fidelity setting**. For this purpose we use NASBench201 benchmark with CIFAR-10 dataset.

Please check [Poster.pdf](https://github.com/ErlisLushtaku/DeepLearningLab-Multi-fidelity_NAS_using_LLMs/blob/main/Poster.pdf) to see a more detailed explanation of the project and the results.
