{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:46.126412Z",
     "start_time": "2024-07-21T17:09:23.169708Z"
    }
   },
   "source": [
    "from nb201 import NB201Benchmark\n",
    "import numpy as np\n",
    "from warmstart.utils_templates import FullTemplate\n",
    "import ConfigSpace as CS\n",
    "from ConfigSpace import Configuration\n",
    "import ollama\n",
    "import torchvision\n",
    "from exp_baselines.bayesmark.data import ProblemType\n",
    "import ast\n",
    "from llambo.llambo import LLAMBO\n",
    "from utils import convert_LLAMBO_df_to_synetune_dict\n",
    "from utils import convert_synetune_dict_to_LLAMBO_compatible_format\n",
    "\n",
    "from syne_tune_local.experiments.benchmark_definitions.nas201 import nas201_benchmark\n",
    "from syne_tune_local.blackbox_repository import BlackboxRepositoryBackend\n",
    "from syne_tune_local.backend.simulator_backend.simulator_callback import SimulatorCallback\n",
    "from syne_tune_local import Tuner, StoppingCriterion\n",
    "\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "import logging\n",
    "from syne_tune_local.optimizer.schedulers import FIFOScheduler\n",
    "from syne_tune_local.optimizer.schedulers.searchers import StochasticAndFilterDuplicatesSearcher"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/e.lushtaku/Library/Application Support/sagemaker/config.yaml\n",
      "Dependencies of YAHPO are not imported since dependencies are missing. You can install them with\n",
      "   pip install 'syne-tune[yahpo]'\n",
      "or (for everything)\n",
      "   pip install 'syne-tune[extra]'\n",
      "Dependencies of YAHPO are not imported since dependencies are missing. You can install them with\n",
      "   pip install 'syne-tune[yahpo]'\n",
      "or (for everything)\n",
      "   pip install 'syne-tune[extra]'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load NB201 Benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "b = NB201Benchmark(path=\"./nb201.pkl\", dataset='cifar10')\n",
    "cs = b.get_configuration_space()\n",
    "config = cs.sample_configuration()  # samples a configuration uniformly at random\n",
    "\n",
    "print(cs)\n",
    "print(\"Numpy representation: \", config.get_array())\n",
    "print(\"Dict representation: \", config.get_dictionary())\n",
    "\n",
    "#configuration from a dict\n",
    "new_config = Configuration(cs, values=config.get_dictionary())\n",
    "print(new_config)\n",
    "\n",
    "y, cost = b.objective_function(config)\n",
    "print(\"Test error: %f %%\" % y)\n",
    "print(\"Runtime %f s\" % cost)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:46.184512Z",
     "start_time": "2024-07-21T17:09:46.128300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration space object:\n",
      "  Hyperparameters:\n",
      "    op_0_to_1, Type: Categorical, Choices: {none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3}, Default: none\n",
      "    op_0_to_2, Type: Categorical, Choices: {none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3}, Default: none\n",
      "    op_0_to_3, Type: Categorical, Choices: {none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3}, Default: none\n",
      "    op_1_to_2, Type: Categorical, Choices: {none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3}, Default: none\n",
      "    op_1_to_3, Type: Categorical, Choices: {none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3}, Default: none\n",
      "    op_2_to_3, Type: Categorical, Choices: {none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3}, Default: none\n",
      "\n",
      "Numpy representation:  [3. 0. 3. 1. 2. 2.]\n",
      "Dict representation:  {'op_0_to_1': 'nor_conv_1x1', 'op_0_to_2': 'none', 'op_0_to_3': 'nor_conv_1x1', 'op_1_to_2': 'skip_connect', 'op_1_to_3': 'avg_pool_3x3', 'op_2_to_3': 'avg_pool_3x3'}\n",
      "Configuration(values={\n",
      "  'op_0_to_1': 'nor_conv_1x1',\n",
      "  'op_0_to_2': 'none',\n",
      "  'op_0_to_3': 'nor_conv_1x1',\n",
      "  'op_1_to_2': 'skip_connect',\n",
      "  'op_1_to_3': 'avg_pool_3x3',\n",
      "  'op_2_to_3': 'avg_pool_3x3',\n",
      "})\n",
      "Test error: 9.075000 %\n",
      "Runtime 2677.989006 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/nhhvw1x54fggv0t8nzr9s4xm0000gp/T/ipykernel_4212/2721149981.py:7: DeprecationWarning: `Configuration` act's like a dictionary. Please use `dict(config)` instead of `get_dictionary` if you explicitly need a `dict`\n",
      "  print(\"Dict representation: \", config.get_dictionary())\n",
      "/var/folders/b0/nhhvw1x54fggv0t8nzr9s4xm0000gp/T/ipykernel_4212/2721149981.py:10: DeprecationWarning: `Configuration` act's like a dictionary. Please use `dict(config)` instead of `get_dictionary` if you explicitly need a `dict`\n",
      "  new_config = Configuration(cs, values=config.get_dictionary())\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Arguments for LLAMBO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "task_context = {\n",
    "    'model': 'CNN',\n",
    "    'task': 'classification',\n",
    "    'tot_feats': 32 * 32 * 3,\n",
    "    'cat_feats': 0,\n",
    "    'num_feat': 32 * 32 * 3,\n",
    "    'n_classes': 10,\n",
    "    'metric': 'loss',\n",
    "    'lower_is_better': True,\n",
    "    'num_samples': 50000,\n",
    "    'hyperparameter_constraints': {\n",
    "        'op_0_to_1': ['categorical', None, [\"none\", \"skip_connect\", \"avg_pool_3x3\", \"nor_conv_1x1\", \"nor_conv_3x3\"]],\n",
    "        # [type, transform, [min_value, max_value]]\n",
    "        'op_0_to_2': ['categorical', None, [\"none\", \"skip_connect\", \"avg_pool_3x3\", \"nor_conv_1x1\", \"nor_conv_3x3\"]],\n",
    "        'op_0_to_3': ['categorical', None, [\"none\", \"skip_connect\", \"avg_pool_3x3\", \"nor_conv_1x1\", \"nor_conv_3x3\"]],\n",
    "        'op_1_to_2': ['categorical', None, [\"none\", \"skip_connect\", \"avg_pool_3x3\", \"nor_conv_1x1\", \"nor_conv_3x3\"]],\n",
    "        'op_1_to_3': ['categorical', None, [\"none\", \"skip_connect\", \"avg_pool_3x3\", \"nor_conv_1x1\", \"nor_conv_3x3\"]],\n",
    "        'op_2_to_3': ['categorical', None, [\"none\", \"skip_connect\", \"avg_pool_3x3\", \"nor_conv_1x1\", \"nor_conv_3x3\"]]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def init_f():\n",
    "    return\n",
    "\n",
    "\n",
    "def eval_point(config):\n",
    "    new_config = Configuration(b.get_configuration_space(), values=config)\n",
    "    res = b.objective_function(new_config)\n",
    "    res_dict = {\n",
    "        \"score\": res[0],\n",
    "        \"train_time\": res[1]\n",
    "    }\n",
    "    return config, res_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:46.188942Z",
     "start_time": "2024-07-21T17:09:46.185405Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ollama"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# chat_engine = \"llama3\"\n",
    "# model = ollama.pull(chat_engine)\n",
    "# response = ollama.chat(model=\"llama3\", messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n",
    "# print(response)\n",
    "# ollama.list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:46.191504Z",
     "start_time": "2024-07-21T17:09:46.189850Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True)\n",
    "\n",
    "\n",
    "def fetch_statistics(dict, dataset):\n",
    "    images = dataset.data\n",
    "    labels = dataset.targets\n",
    "\n",
    "    images_np = np.array(images)\n",
    "    labels_np = np.array(labels)\n",
    "\n",
    "    pixel_mean = np.mean(images_np / 255.)\n",
    "    pixel_std = np.std(images_np / 255.)\n",
    "\n",
    "    class_counts = np.bincount(labels_np)\n",
    "    class_distribution = class_counts / len(labels_np)\n",
    "\n",
    "    dict['pixel_mean'] = pixel_mean\n",
    "    dict['pixel_std'] = pixel_std\n",
    "    dict['class_distribution'] = class_distribution.tolist()\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "task_context = fetch_statistics(task_context, trainset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:47.797196Z",
     "start_time": "2024-07-21T17:09:46.192975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Warmstart"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T21:50:50.906679200Z",
     "start_time": "2024-07-16T21:50:50.879733200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = \"No_Context\"\n",
    "metric = \"acc\"\n",
    "NUM_SEEDS = 10\n",
    "problem_type = ProblemType.clf\n",
    "\n",
    "\n",
    "def extract_configs_from_response(response):\n",
    "    content = response['message']['content']\n",
    "    start = content.find(\"[\")\n",
    "    end = content.rfind(\"]\") + 1\n",
    "    list_str = content[start:end]\n",
    "    configurations = ast.literal_eval(list_str)\n",
    "    return configurations\n",
    "\n",
    "\n",
    "def is_dict_valid_in_config_space(d, config_space):\n",
    "    try:\n",
    "        # Attempt to create a Configuration object with the given dictionary and config space\n",
    "        config = CS.Configuration(config_space, values=d)\n",
    "        return True\n",
    "    except:\n",
    "        # Return False if the dictionary is not valid\n",
    "        return False\n",
    "    # Function to check if all dictionaries in a list are valid in the given configuration space\n",
    "\n",
    "\n",
    "def check_all_list(parsed_dicts, config_space):\n",
    "    for idx, d in enumerate(parsed_dicts):\n",
    "        if not is_dict_valid_in_config_space(d, config_space):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def obtain_all_list_valid(resp, config_space):\n",
    "    if check_all_list(resp, config_space):\n",
    "        return resp\n",
    "    print(\"fail\")\n",
    "\n",
    "\n",
    "def generate_init_conf(n_samples):\n",
    "    template_object = FullTemplate(context=config, provide_ranges=True)\n",
    "    input_prompt = template_object.add_context(config_space=cs, num_recommendation=n_samples, task_dict=task_context)\n",
    "    response = ollama.chat(model=\"llama3\", messages=[{'role': 'user', 'content': input_prompt}])\n",
    "    configs = extract_configs_from_response(response)\n",
    "    return obtain_all_list_valid(configs, cs)\n",
    "\n",
    "#print(generate_init_conf(3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:47.804701Z",
     "start_time": "2024-07-21T17:09:47.800069Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Llambo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T21:50:50.916420200Z",
     "start_time": "2024-07-16T21:50:50.906679200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "llambo = LLAMBO(task_context, sm_mode='discriminative', n_candidates=10, n_templates=2, n_gens=10,\n",
    "                alpha=0.1, n_initial_samples=5, n_trials=4,\n",
    "                init_f=generate_init_conf,\n",
    "                bbox_eval_f=eval_point,\n",
    "                chat_engine=\"llama3\")\n",
    "llambo.seed = 0\n",
    "\n",
    "# run optimization\n",
    "#configs, fvals = llambo.optimize(test_metric=\"score\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:47.809431Z",
     "start_time": "2024-07-21T17:09:47.805781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n",
      "[Search settings]: \n",
      "\tn_candidates: 10, n_templates: 2, n_gens: 10, \n",
      "\talpha: 0.1, n_initial_samples: 5, n_trials: 4, \n",
      "\tusing warping: False, ablation: None, shuffle_features: False\n",
      "[Task]: \n",
      "\ttask type: classification, sm: discriminative, lower is better: True\n",
      "Hyperparameter search space: \n",
      "{'op_0_to_1': ['categorical',\n",
      "               None,\n",
      "               ['none',\n",
      "                'skip_connect',\n",
      "                'avg_pool_3x3',\n",
      "                'nor_conv_1x1',\n",
      "                'nor_conv_3x3']],\n",
      " 'op_0_to_2': ['categorical',\n",
      "               None,\n",
      "               ['none',\n",
      "                'skip_connect',\n",
      "                'avg_pool_3x3',\n",
      "                'nor_conv_1x1',\n",
      "                'nor_conv_3x3']],\n",
      " 'op_0_to_3': ['categorical',\n",
      "               None,\n",
      "               ['none',\n",
      "                'skip_connect',\n",
      "                'avg_pool_3x3',\n",
      "                'nor_conv_1x1',\n",
      "                'nor_conv_3x3']],\n",
      " 'op_1_to_2': ['categorical',\n",
      "               None,\n",
      "               ['none',\n",
      "                'skip_connect',\n",
      "                'avg_pool_3x3',\n",
      "                'nor_conv_1x1',\n",
      "                'nor_conv_3x3']],\n",
      " 'op_1_to_3': ['categorical',\n",
      "               None,\n",
      "               ['none',\n",
      "                'skip_connect',\n",
      "                'avg_pool_3x3',\n",
      "                'nor_conv_1x1',\n",
      "                'nor_conv_3x3']],\n",
      " 'op_2_to_3': ['categorical',\n",
      "               None,\n",
      "               ['none',\n",
      "                'skip_connect',\n",
      "                'avg_pool_3x3',\n",
      "                'nor_conv_1x1',\n",
      "                'nor_conv_3x3']]}\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Searcher"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T21:49:55.022268800Z",
     "start_time": "2024-07-16T21:49:53.636741500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_RETRIES = 100\n",
    "\n",
    "\n",
    "class LlamboSearcher(StochasticAndFilterDuplicatesSearcher):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_space: Dict[str, Any],\n",
    "            metric: Union[List[str], str],\n",
    "            points_to_evaluate: Optional[List[dict]] = None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            config_space,\n",
    "            metric=metric,\n",
    "            points_to_evaluate=points_to_evaluate,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "\n",
    "    def configure_scheduler(self, scheduler):\n",
    "        from syne_tune_local.optimizer.schedulers.scheduler_searcher import (\n",
    "            TrialSchedulerWithSearcher,\n",
    "        )\n",
    "\n",
    "        assert isinstance(\n",
    "            scheduler, TrialSchedulerWithSearcher\n",
    "        ), \"This searcher requires TrialSchedulerWithSearcher scheduler\"\n",
    "        super().configure_scheduler(scheduler)\n",
    "\n",
    "    def _train_model(self, train_data: np.ndarray, train_targets: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        :param train_data: Training input feature matrix X\n",
    "        :param train_targets: Training targets y\n",
    "        :return: Was training successful?\n",
    "        \"\"\"\n",
    "        llambo._update_observations(train_data, train_targets)\n",
    "        return True\n",
    "    \n",
    "    def get_state(self) -> Dict[str, Any]:\n",
    "        return dict(\n",
    "            super().get_state(),\n",
    "        )\n",
    "\n",
    "    def _restore_from_state(self, state: Dict[str, Any]):\n",
    "        super()._restore_from_state(state)\n",
    "\n",
    "    def get_config(self, **kwargs) -> Optional[Dict[str, Any]]:\n",
    "        suggestion = self._next_initial_config()\n",
    "        if suggestion is None:\n",
    "            if self.y:\n",
    "                if self._train_model(np.array(self.X), np.array(self.y)):\n",
    "                    suggestion = convert_LLAMBO_df_to_synetune_dict(llambo.get_config())\n",
    "            \n",
    "        return suggestion\n",
    "        \n",
    "    def _update(self, trial_id: str, config: Dict[str, Any], result: Dict[str, Any]):\n",
    "        self.X.append(convert_synetune_dict_to_LLAMBO_compatible_format(config))\n",
    "        self.y.append(result)\n",
    "        \n",
    "    def clone_from_state(self, state: Dict[str, Any]):\n",
    "        raise NotImplementedError\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:47.815350Z",
     "start_time": "2024-07-21T17:09:47.810305Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T17:09:47.825019Z",
     "start_time": "2024-07-21T17:09:47.816212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_RETRIES = 100\n",
    "\n",
    "\n",
    "class MultiFidelityLLamboSearcher(LlamboSearcher):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_space: Dict[str, Any],\n",
    "            metric: Union[List[str], str],\n",
    "            points_to_evaluate: Optional[List[dict]] = None,\n",
    "            resource_attr: Optional[str] = None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            config_space,\n",
    "            metric=metric,\n",
    "            points_to_evaluate=points_to_evaluate,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.resource_attr = resource_attr\n",
    "        self.resource_levels = []\n",
    "\n",
    "    def configure_scheduler(self, scheduler):\n",
    "        from syne_tune_local.optimizer.schedulers.multi_fidelity import (\n",
    "            MultiFidelitySchedulerMixin,\n",
    "        )\n",
    "\n",
    "        super().configure_scheduler(scheduler)\n",
    "        assert isinstance(\n",
    "            scheduler, MultiFidelitySchedulerMixin\n",
    "        ), \"This searcher requires MultiFidelitySchedulerMixin scheduler\"\n",
    "        self.resource_attr = scheduler.resource_attr\n",
    "       \n",
    "    def _train_model(self, train_data: np.ndarray, train_targets: np.ndarray) -> bool:\n",
    "        highest_resource_level = self._highest_resource_model_can_fit()\n",
    "        if highest_resource_level is None:\n",
    "            return False\n",
    "        else:\n",
    "            indices = np.where(self.resource_levels == highest_resource_level)\n",
    "            sub_data = train_data[indices]\n",
    "            sub_targets = train_targets[indices]\n",
    "        return super()._train_model(sub_data, sub_targets)\n",
    "     \n",
    "    def _highest_resource_model_can_fit(self) -> Optional[int]:\n",
    "        # find the highest resource level we have at least one data points of the positive class\n",
    "        min_data_points = 4\n",
    "        unique_resource_levels, counts = np.unique(\n",
    "            self.resource_levels, return_counts=True\n",
    "        )\n",
    "        idx = np.where(counts >= min_data_points)[0]\n",
    "\n",
    "        if len(idx) == 0:\n",
    "            return None\n",
    "\n",
    "        # collect data on the highest resource level\n",
    "        return unique_resource_levels[idx[-1]]\n",
    "    \n",
    "    def get_state(self) -> Dict[str, Any]:\n",
    "        return dict(\n",
    "            super().get_state(),\n",
    "        )\n",
    "\n",
    "    def _restore_from_state(self, state: Dict[str, Any]):\n",
    "        super()._restore_from_state(state)\n",
    "\n",
    "    def _update(self, trial_id: str, config: Dict, result: Dict):\n",
    "        super()._update(trial_id=trial_id, config=config, result=result)\n",
    "        resource_level = int(result[self.resource_attr])\n",
    "        self.resource_levels.append(resource_level)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple searcher combined with LLAMBO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from syne_tune_local.optimizer.schedulers.synchronous import SynchronousGeometricHyperbandScheduler\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "random_seed = 1\n",
    "nb201_random_seed = 0\n",
    "n_workers = 1\n",
    "dataset_name = \"cifar10\"\n",
    "benchmark = nas201_benchmark(dataset_name)\n",
    "\n",
    "max_resource_attr = benchmark.max_resource_attr\n",
    "trial_backend = BlackboxRepositoryBackend(\n",
    "    blackbox_name=benchmark.blackbox_name,\n",
    "    elapsed_time_attr=benchmark.elapsed_time_attr,\n",
    "    max_resource_attr=max_resource_attr,\n",
    "    dataset=dataset_name,\n",
    "    seed=nb201_random_seed,\n",
    ")\n",
    "\n",
    "blackbox = trial_backend.blackbox\n",
    "nas_configuration_space = blackbox.configuration_space_with_max_resource_attr(\n",
    "    max_resource_attr\n",
    ")\n",
    "\n",
    "points_to_evaluate = llambo.initialize_configs(5)\n",
    "points_to_evaluate = convert_LLAMBO_df_to_synetune_dict(points_to_evaluate)\n",
    "scheduler = SynchronousGeometricHyperbandScheduler(\n",
    "    config_space=nas_configuration_space,\n",
    "    max_resource_attr=max_resource_attr,\n",
    "    mode=benchmark.mode,\n",
    "    metric=benchmark.metric,\n",
    "    random_seed=random_seed,\n",
    "    searcher=MultiFidelityLLamboSearcher,\n",
    "    resource_attr=blackbox.fidelity_name(),\n",
    "    points_to_evaluate=points_to_evaluate,\n",
    ")\n",
    "\n",
    "max_num_trials_started = 5\n",
    "stop_criterion = StoppingCriterion(max_num_trials_started=max_num_trials_started)\n",
    "print_update_interval = 700\n",
    "results_update_interval = 300\n",
    "tuner = Tuner(\n",
    "    trial_backend=trial_backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=n_workers,\n",
    "    sleep_time=0,\n",
    "    results_update_interval=results_update_interval,\n",
    "    print_update_interval=print_update_interval,\n",
    "    callbacks=[SimulatorCallback()],\n",
    ")\n",
    "\n",
    "tuner.run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T17:11:10.401888Z",
     "start_time": "2024-07-21T17:09:47.825895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'op_0_to_1': 'skip_connect', 'op_0_to_2': 'avg_pool_3x3', 'op_0_to_3': 'nor_conv_1x1', 'op_1_to_2': 'none', 'op_1_to_3': 'nor_conv_3x3', 'op_2_to_3': 'skip_connect'}\n",
      " {'op_0_to_1': 'avg_pool_3x3', 'op_0_to_2': 'nor_conv_1x1', 'op_0_to_3': 'none', 'op_1_to_2': 'skip_connect', 'op_1_to_3': 'nor_conv_1x1', 'op_2_to_3': 'avg_pool_3x3'}\n",
      " {'op_0_to_1': 'nor_conv_3x3', 'op_0_to_2': 'skip_connect', 'op_0_to_3': 'avg_pool_3x3', 'op_1_to_2': 'none', 'op_1_to_3': 'skip_connect', 'op_2_to_3': 'nor_conv_1x1'}\n",
      " {'op_0_to_1': 'none', 'op_0_to_2': 'avg_pool_3x3', 'op_0_to_3': 'nor_conv_3x3', 'op_1_to_2': 'skip_connect', 'op_1_to_3': 'avg_pool_3x3', 'op_2_to_3': 'none'}\n",
      " {'op_0_to_1': 'nor_conv_1x1', 'op_0_to_2': 'nor_conv_3x3', 'op_0_to_3': 'skip_connect', 'op_1_to_2': 'avg_pool_3x3', 'op_1_to_3': 'none', 'op_2_to_3': 'nor_conv_1x1'}]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "Adjusted alpha: 0.1 | [original alpha: 0.1], desired fval: 0.525230\n",
      "====================================================================================================\n",
      "EXAMPLE ACQUISITION PROMPT\n",
      "Length of prompt templates: 2\n",
      "Length of query templates: 2\n",
      "The following are examples of performance of a CNN measured in loss and the corresponding model architecture configurations. The model is evaluated on a image classification task containing 10 classes. The dataset contains 50000 images and each image has height 32, width 32, and 3 channels. The allowable choices for the architectures are:\n",
      "- op_0_to_1: [none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3] (categorical)\n",
      "- op_0_to_2: [none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3] (categorical)\n",
      "- op_0_to_3: [none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3] (categorical)\n",
      "- op_1_to_2: [none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3] (categorical)\n",
      "- op_1_to_3: [none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3] (categorical)\n",
      "- op_2_to_3: [none, skip_connect, avg_pool_3x3, nor_conv_1x1, nor_conv_3x3] (categorical)\n",
      "Recommend a configuration that can achieve the target performance of 0.525230. Do not recommend categorical choices outside of given lists. Recommend categorical choices with highest possible precision, as requested by the allowed ranges. Your response must only contain the predicted configuration, in the format ## configuration ##. Please provide a configuration different from the provided ones.\n",
      "\n",
      "Performance: 0.613700\n",
      "Hyperparameter configuration: ## op_0_to_1: nor_conv_3x3, op_0_to_2: skip_connect, op_0_to_3: avg_pool_3x3, op_1_to_2: none, op_1_to_3: skip_connect, op_2_to_3: nor_conv_1x1 ##\n",
      "Performance: 0.626100\n",
      "Hyperparameter configuration: ## op_0_to_1: skip_connect, op_0_to_2: avg_pool_3x3, op_0_to_3: nor_conv_1x1, op_1_to_2: none, op_1_to_3: nor_conv_3x3, op_2_to_3: skip_connect ##\n",
      "Performance: 0.606100\n",
      "Hyperparameter configuration: ## op_0_to_1: avg_pool_3x3, op_0_to_2: nor_conv_1x1, op_0_to_3: none, op_1_to_2: skip_connect, op_1_to_3: nor_conv_1x1, op_2_to_3: avg_pool_3x3 ##\n",
      "Performance: 0.534400\n",
      "Hyperparameter configuration: ## op_0_to_1: none, op_0_to_2: avg_pool_3x3, op_0_to_3: nor_conv_3x3, op_1_to_2: skip_connect, op_1_to_3: avg_pool_3x3, op_2_to_3: none ##\n",
      "Performance: 0.606400\n",
      "Hyperparameter configuration: ## op_0_to_1: nor_conv_1x1, op_0_to_2: nor_conv_3x3, op_0_to_3: skip_connect, op_1_to_2: avg_pool_3x3, op_1_to_3: none, op_2_to_3: nor_conv_1x1 ##\n",
      "Performance: 0.525230\n",
      "Hyperparameter configuration:\n",
      "====================================================================================================\n",
      "Attempt: 0, number of proposed candidate points: 2,  number of accepted candidate points: 2\n",
      "Attempt: 1, number of proposed candidate points: 2,  number of accepted candidate points: 4\n",
      "Attempt: 2, number of proposed candidate points: 2,  number of accepted candidate points: 6\n",
      "======================================================================================================================================================\n",
      "EXAMPLE POINTS PROPOSED\n",
      "      op_0_to_1     op_0_to_2     op_0_to_3     op_1_to_2     op_1_to_3  \\\n",
      "0  skip_connect  nor_conv_3x3  avg_pool_3x3          none  nor_conv_3x3   \n",
      "1  nor_conv_3x3  skip_connect  avg_pool_3x3  nor_conv_3x3          none   \n",
      "2  nor_conv_3x3  skip_connect  avg_pool_3x3  nor_conv_1x1  skip_connect   \n",
      "3  nor_conv_3x3  skip_connect  avg_pool_3x3          none  nor_conv_1x1   \n",
      "4  nor_conv_3x3  skip_connect  avg_pool_3x3          none  nor_conv_3x3   \n",
      "5  nor_conv_3x3  skip_connect  avg_pool_3x3  nor_conv_3x3  skip_connect   \n",
      "\n",
      "      op_2_to_3  \n",
      "0  skip_connect  \n",
      "1  nor_conv_1x1  \n",
      "2  nor_conv_1x1  \n",
      "3  nor_conv_3x3  \n",
      "4  skip_connect  \n",
      "5  nor_conv_1x1  \n",
      "======================================================================================================================================================\n",
      "****************************************************************************************************\n",
      "Number of all_prompt_templates: 2\n",
      "Number of query_examples: 6\n",
      "The following are architecture configurations for a CNN and the corresponding performance measured in loss. The model is evaluated on a image classification task and the label contains 10 classes. The dataset contains 50000 images and each image has height 32, width 32, and 3 channels. Your response should only contain the predicted loss in the format ## performance ##.\n",
      "Hyperparameter configuration: op_0_to_1 is nor_conv_3x3, op_0_to_2 is skip_connect, op_0_to_3 is avg_pool_3x3, op_1_to_2 is none, op_1_to_3 is skip_connect, op_2_to_3 is nor_conv_1x1\n",
      "Performance: ## 0.613700 ##\n",
      "Hyperparameter configuration: op_0_to_1 is skip_connect, op_0_to_2 is avg_pool_3x3, op_0_to_3 is nor_conv_1x1, op_1_to_2 is none, op_1_to_3 is nor_conv_3x3, op_2_to_3 is skip_connect\n",
      "Performance: ## 0.626100 ##\n",
      "Hyperparameter configuration: op_0_to_1 is avg_pool_3x3, op_0_to_2 is nor_conv_1x1, op_0_to_3 is none, op_1_to_2 is skip_connect, op_1_to_3 is nor_conv_1x1, op_2_to_3 is avg_pool_3x3\n",
      "Performance: ## 0.606100 ##\n",
      "Hyperparameter configuration: op_0_to_1 is none, op_0_to_2 is avg_pool_3x3, op_0_to_3 is nor_conv_3x3, op_1_to_2 is skip_connect, op_1_to_3 is avg_pool_3x3, op_2_to_3 is none\n",
      "Performance: ## 0.534400 ##\n",
      "Hyperparameter configuration: op_0_to_1 is nor_conv_1x1, op_0_to_2 is nor_conv_3x3, op_0_to_3 is skip_connect, op_1_to_2 is avg_pool_3x3, op_1_to_3 is none, op_2_to_3 is nor_conv_1x1\n",
      "Performance: ## 0.606400 ##\n",
      "Hyperparameter configuration: op_0_to_1 is skip_connect, op_0_to_2 is nor_conv_3x3, op_0_to_3 is avg_pool_3x3, op_1_to_2 is none, op_1_to_3 is nor_conv_3x3, op_2_to_3 is skip_connect\n",
      "Performance: \n",
      "======================================================================================================================================================\n",
      "SELECTED CANDIDATE POINT\n",
      "      op_0_to_1     op_0_to_2     op_0_to_3     op_1_to_2     op_1_to_3  \\\n",
      "2  nor_conv_3x3  skip_connect  avg_pool_3x3  nor_conv_1x1  skip_connect   \n",
      "\n",
      "      op_2_to_3  \n",
      "2  nor_conv_1x1  \n",
      "======================================================================================================================================================\n",
      "--------------------\n",
      "Resource summary (last result is reported):\n",
      " trial_id     status  iter        hp_x0        hp_x1        hp_x2        hp_x3        hp_x4        hp_x5  epochs  metric_valid_error  metric_train_error  metric_runtime  metric_elapsed_time  metric_latency  metric_flops  metric_params  hp_epoch\n",
      "        0     Paused     1 skip_connect avg_pool_3x3 nor_conv_1x1         none nor_conv_3x3 skip_connect       1              0.6261             0.62102       17.225590            17.225590        0.015183     47.104649       0.344346       1.0\n",
      "        1     Paused     1 avg_pool_3x3 nor_conv_1x1         none skip_connect nor_conv_1x1 avg_pool_3x3       1              0.6061             0.61306       18.179995            18.179995        0.015190     15.647370       0.129306       1.0\n",
      "        2     Paused     1 nor_conv_3x3 skip_connect avg_pool_3x3         none skip_connect nor_conv_1x1       1              0.6137             0.66596       17.789129            17.789129        0.014873     47.104649       0.344346       1.0\n",
      "        3     Paused     1         none avg_pool_3x3 nor_conv_3x3 skip_connect avg_pool_3x3         none       1              0.5344             0.57066       15.678905            15.678905        0.013592     43.172489       0.316346       1.0\n",
      "        4     Paused     1 nor_conv_1x1 nor_conv_3x3 skip_connect avg_pool_3x3         none nor_conv_1x1       1              0.6064             0.59958       21.133892            21.133892        0.017894     51.036812       0.372346       1.0\n",
      "        5 InProgress     0 nor_conv_3x3 skip_connect avg_pool_3x3 nor_conv_1x1 skip_connect nor_conv_1x1       1                   -                   -               -                    -               -             -              -         -\n",
      "1 trials running, 5 finished (0 until the end), 61.25s wallclock-time\n",
      "\n",
      "metric_valid_error: best 0.5343999862670898 for trial-id 3\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
