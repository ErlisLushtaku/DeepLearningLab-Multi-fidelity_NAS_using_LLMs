{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nb201 import NB201Benchmark\n",
    "import numpy as np\n",
    "import openai\n",
    "from warmstart.utils_templates import FullTemplate\n",
    "import ConfigSpace as CS\n",
    "import google.generativeai as genai\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load NB201 Benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b = NB201Benchmark(path=\"./nb201.pkl\", dataset='cifar10')\n",
    "cs = b.get_configuration_space()\n",
    "config = cs.sample_configuration()  # samples a configuration uniformly at random\n",
    "\n",
    "print(cs)\n",
    "print(\"Numpy representation: \", config.get_array())\n",
    "print(\"Dict representation: \", config.get_dictionary())\n",
    "\n",
    "y, cost = b.objective_function(config)\n",
    "print(\"Test error: %f %%\" % y)\n",
    "print(\"Runtime %f s\" % cost)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Arguments for LLAMBO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_context = {\n",
    "    'model': 'CNN',\n",
    "    'task': 'classification',\n",
    "    'tot_feats': 32 * 32 * 3,\n",
    "    'cat_feats': 0,\n",
    "    'num_feats': 32 * 32 * 3,\n",
    "    'n_classes': 10,\n",
    "    'metric': 'accuracy',\n",
    "    'lower_is_better': False,\n",
    "    'num_samples': 50000,\n",
    "    'hyperparameter_constraints': {\n",
    "        'max_depth': ['int', 'linear', [1, 15]],  # [type, transform, [min_value, max_value]]\n",
    "        'max_features': ['float', 'logit', [0.01, 0.99]],\n",
    "        'min_impurity_decrease': ['float', 'linear', [0.0, 0.5]],\n",
    "        'min_samples_leaf': ['float', 'logit', [0.01, 0.49]],\n",
    "        'min_samples_split': ['float', 'logit', [0.01, 0.99]],\n",
    "        'min_weight_fraction_leaf': ['float', 'logit', [0.01, 0.49]]\n",
    "    }\n",
    "}\n",
    "\n",
    "#me bo ni mapping t ints n operations n NAS\n",
    "a = {\n",
    "    \"op_0_to_1\": ['int', 'linear', [1, 15]],\n",
    "    \"op_0_to_2\": 2,\n",
    "    \"op_0_to_3\": 2,\n",
    "    \"op_1_to_2\": 2,\n",
    "    \"op_1_to_3\": 2,\n",
    "    \"op_2_to_3\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "#qetu me define warmstarting\n",
    "def init_f():\n",
    "    return\n",
    "\n",
    "\n",
    "#should use the NB201 benchmark eval\n",
    "def eval_point():\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test openai"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#openai.api_type = \"V1\"\n",
    "openai.api_version = \"V1\"\n",
    "#openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = \"sk-proj-RfrSK7Ivg1Sdtz7SKQmhT3BlbkFJ2FNpiTEx5c0LC3YlyUgB\"\n",
    "\n",
    "prompt = \"Once upon a time in a magical forest, there was a\"\n",
    "\n",
    "resp = await openai.Completion.acreate(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    prompt=prompt,\n",
    "    temperature=0.7,\n",
    "    max_tokens=5,\n",
    "    top_p=0.95,\n",
    "    request_timeout=10,\n",
    "    logprobs=5,\n",
    ")\n",
    "print(resp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test gemini api"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "genai.configure(api_key='AIzaSyBR5QDQhgX1CulOKYZSSFei7dv4QQ4yFXI')\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "prompt = \"Do these look store-bought or homemade?\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test gemma2b-it from huggingface"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_length=512)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
